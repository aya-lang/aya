require la {dot}

.# Sigmoid helper functions
{-1 * Me 1+ 1\/}:sigmoid;
{:& 1\- *}:dsigmoid;

.# Neural Net class
class nn

def nn::__init__ {x y self,
    x self.:input;
    y self.:y;
    0 yE L self.:output;
    .# Randomly initialize weights
    0 [x:E1I 4] L {;.Q} .O self.:weights_a;
    0 [4     1] L {;.Q} .O self.:weights_b;
}


def nn::feedforward {self : sigmoid^,
    self.input   self.weights_a dot sigmoid self.:layer_a;
    self.layer_a self.weights_b dot sigmoid self.:output;
}


def nn::backprop {self : dsigmoid^ d d_weights_a d_weights_b,
    self.y self.output - 2 * self.output dsigmoid * :d;

    self.layer_a.T d dot :d_weights_b;

    self.input.T
    d self.weights_b.T dot self.layer_a dsigmoid *
    dot :d_weights_a;

    self.weights_a d_weights_a + self.:weights_a;
    self.weights_b d_weights_b + self.:weights_b;
}


def nn::loss {self,
    self.y self.output - 2^
    .F :& E \W\/ .# mean
}

main {
    import ::plot

    .# Data
    [[0 0 1][0 1 1][1 0 1][1 1 1]] :x;
    [[0][1][1][0]] :y;

    x y nn! :net;
    []:loss;

    .# Train for 1000 steps
    {
        net.feedforward
        net.backprop
        net.loss :m loss .B ;
    } 1000 %

    lossER loss plot.line :plt;
    "Neural Net Example" plt.:title;
    "Loss" plt.:ylabel;
    "Iteration" plt.x.:label;
    plt.view
}
